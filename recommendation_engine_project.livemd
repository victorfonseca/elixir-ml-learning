<!-- livebook:{"persist_outputs":true} -->

# Recommendation Engine Project

```elixir
Mix.install([
  {:nx, "~> 0.7"},
  {:axon, "~> 0.6"},
  {:explorer, "~> 0.8"},
  {:kino, "~> 0.13"},
  {:vega_lite, "~> 0.1"},
  {:httpoison, "~> 2.2"},
  {:csv, "~> 3.2"},
  {:kino_vega_lite, "~> 0.1.11"}
])

alias Explorer.DataFrame
alias Explorer.Series
alias VegaLite, as: Vl

defmodule DataLoader do
  def download_movielens_small do
    url = "https://files.grouplens.org/datasets/movielens/ml-latest-small.zip"
    {:ok, %{body: body}} = HTTPoison.get(url)

    {:ok, files} = :zip.unzip(body, [:memory])

    ratings_csv =
      Enum.find_value(files, fn {name, content} ->
        name = to_string(name)
        if String.ends_with?(name, "ratings.csv"), do: content
      end)

    ratings_csv
  end
end

# Download and load the dataset
ratings_csv = DataLoader.download_movielens_small()
ratings_df = DataFrame.load_csv!(ratings_csv)

# Display the first few rows
IO.inspect(DataFrame.head(ratings_df), label: "First few rows of ratings")

# Print some information about the dataset
IO.puts("Number of rows: #{DataFrame.n_rows(ratings_df)}")
IO.puts("Number of columns: #{DataFrame.n_columns(ratings_df)}")
IO.inspect(DataFrame.names(ratings_df), label: "Column names")
```

## Preprocessing Data

```elixir
# Preprocess the data
users = DataFrame.pull(ratings_df, "userId")
movies = DataFrame.pull(ratings_df, "movieId")
ratings = DataFrame.pull(ratings_df, "rating")

n_users = users |> Series.distinct() |> Series.count()
n_movies = movies |> Series.distinct() |> Series.count()

# Convert to tensors
x_users = Nx.tensor(Series.to_list(users))
x_movies = Nx.tensor(Series.to_list(movies))
y_ratings = Nx.tensor(Series.to_list(ratings))

# Print some information about the preprocessed data
IO.puts("Number of unique users: #{n_users}")
IO.puts("Number of unique movies: #{n_movies}")
IO.puts("Shape of x_users tensor: #{inspect(Nx.shape(x_users))}")
IO.puts("Shape of x_movies tensor: #{inspect(Nx.shape(x_movies))}")
IO.puts("Shape of y_ratings tensor: #{inspect(Nx.shape(y_ratings))}")
```

<!-- livebook:{"output":true} -->

```
Number of unique users: 610
Number of unique movies: 9724
Shape of x_users tensor: {100836}
Shape of x_movies tensor: {100836}
Shape of y_ratings tensor: {100836}
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Training and testing sets data

```elixir
# Split the data into training and testing sets
train_ratio = 0.8
train_size = floor(Nx.size(x_users) * train_ratio)

{x_users_train, x_users_test} = Nx.split(x_users, train_size)
{x_movies_train, x_movies_test} = Nx.split(x_movies, train_size)
{y_ratings_train, y_ratings_test} = Nx.split(y_ratings, train_size)

# Print information about the split
IO.puts("Shape of x_users_train: #{inspect(Nx.shape(x_users_train))}")
IO.puts("Shape of x_users_test: #{inspect(Nx.shape(x_users_test))}")
IO.puts("Shape of x_movies_train: #{inspect(Nx.shape(x_movies_train))}")
IO.puts("Shape of x_movies_test: #{inspect(Nx.shape(x_movies_test))}")
IO.puts("Shape of y_ratings_train: #{inspect(Nx.shape(y_ratings_train))}")
IO.puts("Shape of y_ratings_test: #{inspect(Nx.shape(y_ratings_test))}")
```

<!-- livebook:{"output":true} -->

```
Shape of x_users_train: {80668}
Shape of x_users_test: {20168}
Shape of x_movies_train: {80668}
Shape of x_movies_test: {20168}
Shape of y_ratings_train: {80668}
Shape of y_ratings_test: {20168}
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Prepare Data

```elixir
# Prepare the data
users = DataFrame.pull(ratings_df, "userId")
movies = DataFrame.pull(ratings_df, "movieId")
ratings = DataFrame.pull(ratings_df, "rating")

n_users = users |> Series.distinct() |> Series.count()
n_movies = movies |> Series.distinct() |> Series.count()

# Convert to tensors
x_users = Nx.tensor(Series.to_list(users))
x_movies = Nx.tensor(Series.to_list(movies))
y_ratings = Nx.tensor(Series.to_list(ratings))

# Print some information
IO.puts("Number of unique users: #{n_users}")
IO.puts("Number of unique movies: #{n_movies}")
IO.puts("Shape of ratings tensor: #{inspect(Nx.shape(y_ratings))}")
```

<!-- livebook:{"output":true} -->

```
Number of unique users: 610
Number of unique movies: 9724
Shape of ratings tensor: {100836}
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Matrix Model

```elixir
# Create mappings for user IDs and movie IDs
user_id_map = users |> Series.distinct() |> Series.to_list() |> Enum.with_index() |> Map.new()
movie_id_map = movies |> Series.distinct() |> Series.to_list() |> Enum.with_index() |> Map.new()

n_users = map_size(user_id_map)
n_movies = map_size(movie_id_map)

IO.puts("Number of unique users: #{n_users}")
IO.puts("Number of unique movies: #{n_movies}")


defmodule MatrixFactorization do
  import Nx.Defn

  def init_params(n_users, n_movies, n_factors) do
    user_factors = random_tensor({n_users, n_factors}, 1.0)
    movie_factors = random_tensor({n_movies, n_factors}, 1.0)
    {user_factors, movie_factors}
  end

  def random_tensor(shape, scale) do
    size = Tuple.product(shape)
    data = for _ <- 1..size, do: (:rand.uniform() - 0.5) * scale
    Nx.tensor(data, type: {:f, 32}) |> Nx.reshape(shape)
  end

  defn predict({user_factors, movie_factors}, user_ids, movie_ids) do
    users = Nx.take(user_factors, user_ids)
    movies = Nx.take(movie_factors, movie_ids)
    Nx.sum(users * movies, axes: [1])
  end

  defn loss({user_factors, movie_factors}, user_ids, movie_ids, ratings, lambda) do
    pred = predict({user_factors, movie_factors}, user_ids, movie_ids)
    mse = Nx.mean(Nx.pow(pred - ratings, 2))
    reg = lambda * (Nx.sum(Nx.pow(user_factors, 2)) + Nx.sum(Nx.pow(movie_factors, 2)))
    mse + reg
  end

  defn update({user_factors, movie_factors}, user_ids, movie_ids, ratings, lr, lambda) do
    grad = grad({user_factors, movie_factors}, &loss(&1, user_ids, movie_ids, ratings, lambda))
    {
      user_factors - lr * elem(grad, 0),
      movie_factors - lr * elem(grad, 1)
    }
  end
end

```

<!-- livebook:{"output":true} -->

```
Number of unique users: 610
Number of unique movies: 9724
```

<!-- livebook:{"output":true} -->

```
{:module, MatrixFactorization, <<70, 79, 82, 49, 0, 0, 22, ...>>, true}
```

## Model Train

```elixir
# Data Preprocessing
user_id_list = users |> Series.distinct() |> Series.to_list()
movie_id_list = movies |> Series.distinct() |> Series.to_list()

user_id_map = Enum.with_index(user_id_list) |> Map.new(fn {id, index} -> {id, index} end)
movie_id_map = Enum.with_index(movie_id_list) |> Map.new(fn {id, index} -> {id, index} end)

n_users = map_size(user_id_map)
n_movies = map_size(movie_id_map)

# Convert to tensors using the mappings
x_users = Nx.tensor(Enum.map(Series.to_list(users), &Map.get(user_id_map, &1)))
x_movies = Nx.tensor(Enum.map(Series.to_list(movies), &Map.get(movie_id_map, &1)))

# Normalize ratings to [0, 1] range
{min_rating, max_rating} = Enum.min_max(Series.to_list(ratings))
y_ratings = Nx.tensor(Enum.map(Series.to_list(ratings), fn r -> (r - min_rating) / (max_rating - min_rating) end))

# Model parameters
n_factors = 50
initial_learning_rate = 0.2
decay_rate = 0.95
n_epochs = 50
batch_size = 8192
lambda = 0.01  # L2 regularization strength

# Use full dataset instead of sampling
x_users_train = x_users
x_movies_train = x_movies
y_ratings_train = y_ratings

# Initialize model
params = MatrixFactorization.init_params(n_users, n_movies, n_factors)

# Training loop with learning rate decay
final_params = Enum.reduce(1..n_epochs, params, fn epoch, current_params ->
  current_lr = initial_learning_rate * :math.pow(decay_rate, epoch - 1)
  
  {total_loss, batch_count, new_params} =
    Enum.reduce(0..(Nx.size(x_users_train) - 1)//batch_size, {Nx.tensor(0.0), 0, current_params}, fn i, {acc_loss, acc_count, params} ->
      batch_size = min(batch_size, Nx.size(x_users_train) - i)
      batch_users = Nx.slice(x_users_train, [i], [batch_size])
      batch_movies = Nx.slice(x_movies_train, [i], [batch_size])
      batch_ratings = Nx.slice(y_ratings_train, [i], [batch_size])
      
      new_params = MatrixFactorization.update(params, batch_users, batch_movies, batch_ratings, current_lr, lambda)
      loss = MatrixFactorization.loss(new_params, batch_users, batch_movies, batch_ratings, lambda)
      {Nx.add(acc_loss, loss), acc_count + 1, new_params}
    end)

  avg_loss = Nx.to_number(Nx.divide(total_loss, batch_count))
  IO.puts("Epoch #{epoch}, Average Loss: #{avg_loss}, Learning Rate: #{current_lr}")
  
  new_params
end)

IO.puts("Training completed!")

# Evaluate on full dataset
full_loss = MatrixFactorization.loss(final_params, x_users, x_movies, y_ratings, lambda)
IO.puts("Full Dataset Loss: #{Nx.to_number(full_loss)}")

# Function to make predictions
predict_rating = fn user_id, movie_id ->
  user_index = Map.get(user_id_map, user_id)
  movie_index = Map.get(movie_id_map, movie_id)
  prediction = MatrixFactorization.predict(final_params, Nx.tensor([user_index]), Nx.tensor([movie_index]))
  denormalized = Nx.multiply(prediction, max_rating - min_rating) |> Nx.add(min_rating)
  Nx.to_number(Nx.squeeze(denormalized))
end

# Example predictions
IO.puts("Example predictions:")
IO.puts("User #{Enum.at(user_id_list, 0)}, Movie #{Enum.at(movie_id_list, 0)}: #{predict_rating.(Enum.at(user_id_list, 0), Enum.at(movie_id_list, 0))}")
IO.puts("User #{Enum.at(user_id_list, 0)}, Movie #{Enum.at(movie_id_list, 99)}: #{predict_rating.(Enum.at(user_id_list, 0), Enum.at(movie_id_list, 99))}")
IO.puts("User #{Enum.at(user_id_list, 99)}, Movie #{Enum.at(movie_id_list, 0)}: #{predict_rating.(Enum.at(user_id_list, 99), Enum.at(movie_id_list, 0))}")

```

<!-- livebook:{"output":true} -->

```
Epoch 1, Average Loss: 408.76544189453125, Learning Rate: 0.2
Epoch 2, Average Loss: 369.2974853515625, Learning Rate: 0.19
Epoch 3, Average Loss: 335.3540344238281, Learning Rate: 0.1805
Epoch 4, Average Loss: 306.01458740234375, Learning Rate: 0.171475
Epoch 5, Average Loss: 280.533447265625, Learning Rate: 0.16290125
Epoch 6, Average Loss: 258.30291748046875, Learning Rate: 0.15475618749999998
Epoch 7, Average Loss: 238.82484436035156, Learning Rate: 0.14701837812499996
Epoch 8, Average Loss: 221.68881225585938, Learning Rate: 0.13966745921874996
Epoch 9, Average Loss: 206.55491638183594, Learning Rate: 0.13268408625781244
Epoch 10, Average Loss: 193.14007568359375, Learning Rate: 0.1260498819449218
Epoch 11, Average Loss: 181.20755004882812, Learning Rate: 0.11974738784767575
Epoch 12, Average Loss: 170.55845642089844, Learning Rate: 0.11376001845529193
Epoch 13, Average Loss: 161.02484130859375, Learning Rate: 0.10807201753252733
Epoch 14, Average Loss: 152.46441650390625, Learning Rate: 0.10266841665590097
Epoch 15, Average Loss: 144.75604248046875, Learning Rate: 0.09753499582310592
Epoch 16, Average Loss: 137.79627990722656, Learning Rate: 0.09265824603195061
Epoch 17, Average Loss: 131.49635314941406, Learning Rate: 0.08802533373035308
Epoch 18, Average Loss: 125.77989196777344, Learning Rate: 0.08362406704383542
Epoch 19, Average Loss: 120.58089447021484, Learning Rate: 0.07944286369164365
Epoch 20, Average Loss: 115.84219360351562, Learning Rate: 0.07547072050706145
Epoch 21, Average Loss: 111.51406860351562, Learning Rate: 0.07169718448170838
Epoch 22, Average Loss: 107.55310821533203, Learning Rate: 0.06811232525762297
Epoch 23, Average Loss: 103.92142486572266, Learning Rate: 0.0647067089947418
Epoch 24, Average Loss: 100.58570861816406, Learning Rate: 0.06147137354500472
Epoch 25, Average Loss: 97.51663970947266, Learning Rate: 0.05839780486775448
Epoch 26, Average Loss: 94.68840026855469, Learning Rate: 0.055477914624366756
Epoch 27, Average Loss: 92.07809448242188, Learning Rate: 0.05270401889314841
Epoch 28, Average Loss: 89.66547393798828, Learning Rate: 0.05006881794849099
Epoch 29, Average Loss: 87.43245697021484, Learning Rate: 0.047565377051066435
Epoch 30, Average Loss: 85.36300659179688, Learning Rate: 0.04518710819851311
Epoch 31, Average Loss: 83.44274139404297, Learning Rate: 0.04292775278858746
Epoch 32, Average Loss: 81.6588134765625, Learning Rate: 0.04078136514915808
Epoch 33, Average Loss: 79.99971771240234, Learning Rate: 0.038742296891700174
Epoch 34, Average Loss: 78.45502471923828, Learning Rate: 0.036805182047115165
Epoch 35, Average Loss: 77.01543426513672, Learning Rate: 0.034964922944759405
Epoch 36, Average Loss: 75.67249298095703, Learning Rate: 0.03321667679752143
Epoch 37, Average Loss: 74.41857147216797, Learning Rate: 0.03155584295764536
Epoch 38, Average Loss: 73.24674987792969, Learning Rate: 0.029978050809763093
Epoch 39, Average Loss: 72.1507568359375, Learning Rate: 0.028479148269274935
Epoch 40, Average Loss: 71.12488555908203, Learning Rate: 0.027055190855811186
Epoch 41, Average Loss: 70.16393280029297, Learning Rate: 0.025702431313020625
Epoch 42, Average Loss: 69.26317596435547, Learning Rate: 0.024417309747369595
Epoch 43, Average Loss: 68.41825103759766, Learning Rate: 0.023196444260001114
Epoch 44, Average Loss: 67.62521362304688, Learning Rate: 0.022036622047001058
Epoch 45, Average Loss: 66.88041687011719, Learning Rate: 0.020934790944651005
Epoch 46, Average Loss: 66.18052673339844, Learning Rate: 0.01988805139741845
Epoch 47, Average Loss: 65.52247619628906, Learning Rate: 0.01889364882754753
Epoch 48, Average Loss: 64.90345001220703, Learning Rate: 0.01794896638617015
Epoch 49, Average Loss: 64.32083892822266, Learning Rate: 0.01705151806686164
Epoch 50, Average Loss: 63.77224349975586, Learning Rate: 0.01619894216351856
Training completed!
Full Dataset Loss: 63.52693176269531
Example predictions:
User 1, Movie 1: 0.9534629583358765
User 1, Movie 1552: 0.23956403136253357
User 100, Movie 1: 0.4093063473701477
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Visualizations using VegaLite

```elixir
alias VegaLite, as: Vl
require Kino.VegaLite

# Function to get top N recommendations
get_top_n_recommendations = fn user_id, n ->
  movie_id_map
  |> Enum.map(fn {movie_id, _} ->
    {movie_id, predict_rating.(user_id, movie_id)}
  end)
  |> Enum.sort_by(&elem(&1, 1), :desc)
  |> Enum.take(n)
end

# Get recommendations for the first user
user_id = Enum.at(user_id_list, 0)
top_10_recommendations = get_top_n_recommendations.(user_id, 10)

# Prepare data for visualization
recommendation_data = Enum.map(top_10_recommendations, fn {movie_id, rating} ->
  %{movie_id: to_string(movie_id), predicted_rating: rating}
end)

# Create the visualization
graph = Vl.new(width: 400, height: 300)
|> Vl.data_from_values(recommendation_data)
|> Vl.mark(:bar)
|> Vl.encode_field(:x, "movie_id", type: :nominal, title: "Movie ID")
|> Vl.encode_field(:y, "predicted_rating", type: :quantitative, title: "Predicted Rating")
|> Vl.encode_field(:color, "predicted_rating", type: :quantitative)
|> Vl.config(title: %{text: "Top 10 Movie Recommendations for User #{user_id}"})

# Display the graph
Kino.VegaLite.new(graph)
```
